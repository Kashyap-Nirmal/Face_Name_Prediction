# Face-Name-prediction

[![Hello programmer Welcome](https://img.shields.io/badge/Hello,Programmer!-Welcome-orange.svg?style=flat&logo=github)](https://github.com/Kashyap-Nirmal)
![Lines of code](https://img.shields.io/tokei/lines/github/Kashyap-Nirmal/indian-face-name-prediction?style=plastic)
[![Connect on LinkedIn](https://img.shields.io/badge/--linkedin?label=LinkedIn&logo=LinkedIn&style=social)](https://www.linkedin.com/in/kashyap-nirmal/) 
[![Connect on Gmail](https://img.shields.io/badge/--Gmail?label=Gmail&logo=Gmail&style=social)](mailto:kashyapnirmal18@gmail.com)
![Last Commit](https://img.shields.io/github/last-commit/Kashyap-Nirmal/indian-face-name-prediction?style=plastic)

<p align="center">
<img src="https://capsule-render.vercel.app/api?type=rect&color=gradient&height=100&section=header&text=Indian%20face%20name%20prediction&fontSize=60&fontAlignY=70" /> 
</p>

## Introduction
- I am Kashyap Nirmal, studying in M.Tech second year at [`DAIICT`](https://daiict.ac.in/). I am currently doing my M.Tech Thesis under [`Prof. Manish Gupta`](https://www.guptalab.org/mankg/public_html/). This is the part of my M.Tech thesis. The saved model here is **XGBoost**. The overall prediction accuracy is **44.44%** for 5 classes. The 5 classes are **Krishna, Pranav, Priya, Rahul, Sonal**.

# Problem Statement
- Predict the first name of a person from the facial image of the person. The problem here is assumed to be a **classification problem**. The Classification problem means identifying a category from the available classes. The system predicts the class label which accommodates the new training sample. This task is supervised learning as the class labels are predefined. Thus the final output label will also be from the same set of these class names used in the dataset.

# Ensemble Learning and Boosting
- The ensemble learning technique combines various weak learners to achieve higher prediction accuracy. These weak learners can be any classifier algorithm. Some of the commonly used learners are Random forests. A few of the commonly used Ensemble learning techniques are bagging and boosting.
- In boosting technique, classifiers are used in an iterative manner. After each iteration, the weights for the misclassified samples are increased, and the weights for the correctly classified samples are decreased. This is done in order to provide more importance to the misclassified samples. And thus, it learns from its previous mistakes. XGBoost and AdaBoost are boosting algorithms.

# Confusion Matrix

![Confusion Matrix](https://github.com/Kashyap-Nirmal/indian-face-name-prediction/blob/main/44.44_Indian_XGBoost_CM.png)

## Technology stack
- Python 3

# Important Links
- [`Google Form`](https://forms.gle/7Ez75P7RuunZMQyo6/)

- [`Name100 Dataset`](https://purl.stanford.edu/tp945cq9122/)

- [`All the Confusion Matrices`](https://docs.google.com/presentation/d/1ca1Bc9wPzgsABDhRIupe-HTFolChkBE4L7EoT1SXNkE/edit?usp=sharing)

# Thesis Duration
Aug'21 - Present

<br>Creator [`Kashyap Nirmal`](https://github.com/Kashyap-Nirmal/)

## Note
- Some things added in the README.md file may be borrowed from some other repositories. 
- `Credit to the rightful owner.`
